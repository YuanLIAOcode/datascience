{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Clustering $\\def\\*#1{\\mathbf{#1}}$ $\\DeclareMathOperator*{\\argmax}{arg\\,max}$ $\\DeclareMathOperator*{\\argmin}{arg\\,min}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"Given a set of data points, partition them into groups containing very similar data points.\"* [Aggarwal, 2015]\n",
    "\n",
    "This traditional **optimization problem** consists of partitioning a dataset, $D = \\{\\*x_i\\}_{i=1}^n$, into $k$ groups called **custers** with respect to a given **objective function**. This resulting **clustering** is denoted by $\\mathcal{C} = \\{C_1, C_2,\\ldots,C_k\\}$.\n",
    "\n",
    "Broadly speaking, this problem is known to be NP-Complete. Therefore, there is no polytnomial time algorithm for solving it unless $P$ is equal to $NP$. Basically, this means that there is **no known polynomial time algorithm** for solving it and that there is little chance of finding one. Theresefore, we usualy only consider **heuristics**, **meta-heuristics** or **approximation algorithms** for solving this optimization problem. Such methods, when used with care, can lead to find a *good* solution, but not necessearily an optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.datasets as sk_data\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "cov = [[2, 0],\n",
    "       [0, 2]]\n",
    "mean = (1, 1)\n",
    "D1 = np.random.multivariate_normal(mean, cov, n)\n",
    "\n",
    "cov = [[10, 0],\n",
    "       [0, 10]]\n",
    "mean = (20, 10)\n",
    "D2 = np.random.multivariate_normal(mean, cov, n)\n",
    "\n",
    "cov = [[70, 30],\n",
    "       [30, 20]]\n",
    "mean = (0, 20)\n",
    "D3 = np.random.multivariate_normal(mean, cov, n)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(D1[:,0], D1[:,1], 'ob', D2[:,0], D2[:,1], 'or', D3[:,0], D3[:,1], 'og')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhaustive Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def S(n, k):\n",
    "    \"\"\" Stirling numbers of the second kind \"\"\"\n",
    "    assert n >= 0 and k >= 0\n",
    "    \n",
    "    if k > n:\n",
    "        return 0\n",
    "    \n",
    "    if n == 0 and k == 0:\n",
    "        return 1\n",
    "    \n",
    "    if k == 0:\n",
    "        return 0\n",
    "    \n",
    "    return k * S(n - 1, k) + S(n - 1, k - 1)\n",
    "\n",
    "vS = np.vectorize(S)\n",
    "\n",
    "k_values = range(0, 5)\n",
    "n = 15\n",
    "numbers = vS(n, k_values)\n",
    "\n",
    "print(numbers)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(numbers, color='red')\n",
    "ax.set_ylabel(f'S({n}, k)')\n",
    "ax.set_xlabel('k')\n",
    "ax.set_xticks(range(0, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose is to determine a set of $k$ representatives $\\*y_1, \\ldots, \\*y_k$ that minimize the following **objective function** :\n",
    "\n",
    "$$\n",
    "\\sum_{i = 1}^n \\min\\{d(\\*x_i, \\*y_j) : j = 1, \\ldots, k\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Clustering with k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Select $k$ points as initial cluster centers $C_1, \\ldots, C_k$.\n",
    "* Repeat until convergence :\n",
    "    * For $1 \\leq i \\leq n$, map point $p_i$ to its nearest cluster center $C_j$\n",
    "    * Compute centroid $C_j'$ of the points nearest $C_j$ , for $1 \\leq j \\leq k$\n",
    "    * For all $1 \\leq j \\leq k$, set $C_j = C_j'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![The iterations of k-means (for k = 3)](figures/kmeans_iterations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = sk_data.make_blobs(n_samples=100, centers=4, n_features=2, center_box=(-10.0, 10.0), random_state=0, cluster_std=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the pairwise distances for visualization purposes\n",
    "\n",
    "\n",
    "We can compute pairwise distances using the **sklearn.metrics** functions summarized here:\n",
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "euclidean_dists = metrics.euclidean_distances(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the data using the heatmap of pairwise distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distance_matrix(D):\n",
    "    fig, ax = plt.subplots()\n",
    "    image = ax.matshow(D)\n",
    "    fig.colorbar(image)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    fig.suptitle('Distance Matrix')\n",
    "    \n",
    "plot_distance_matrix(euclidean_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering data using  k-means clustering with [sklearn.cluster ](http://scikit-learn.org/stable/modules/clustering.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(init='k-means++', n_clusters=4, n_init=1)\n",
    "kmeans.fit_predict(X)\n",
    "centroids = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "error = kmeans.inertia_\n",
    "\n",
    "print(\"The total error of the clustering is: \", error)\n",
    "print('\\nCluster Labels')\n",
    "print(labels)\n",
    "print('\\nCluster Centroids')\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 functions in all the clustering classes, **fit()** **predict()** and **fit_predict()** :\n",
    "\n",
    "* **fit()** is building the model from the training data (e.g. finding the centroids),             \n",
    "* **predict()** is assigning labels to test data after building the model,           \n",
    "* **fit_predict()** is doing both in the same data (e.g in kmeans, it finds the centroids and assigns the labels to the dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the results of clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "markers = '^os+'\n",
    "colors = ['r', 'g', 'b', 'Black']\n",
    "for i, m in enumerate(markers):\n",
    "    points = (y == i)\n",
    "    c = [colors[l] for l in labels[points]]\n",
    "    ax.scatter(X[points,0], X[points,1], c=c, marker=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rearrange so that all same labels are consecutive\n",
    "idx = np.argsort(labels)\n",
    "rearranged_dists = euclidean_dists[idx,:][:,idx]\n",
    "plot_distance_matrix(rearranged_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding the number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the error function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error = np.zeros(11)\n",
    "error[0] = 0\n",
    "k_values = range(1,11)\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10)\n",
    "    kmeans.fit_predict(X)\n",
    "    error[k] = kmeans.inertia_\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(1,len(error)),error[1:])\n",
    "ax.set_xlabel('Number of clusters')\n",
    "ax.set_ylabel('Error')\n",
    "ax.set_xticks(k_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $a$ be the mean distance between a sample and all other points in the same class and $b$ be the mean distance between a sample and all other points in the next nearest cluster. Then the \n",
    "**Silhoeutte Coefficient** for a clustering is:\n",
    "$$s = \\frac{b - a}{\\max(a, b)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = metrics.silhouette_score(X, labels, metric='euclidean')\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sc_evaluate_clusters(X, max_clusters):\n",
    "    s = np.zeros(max_clusters+1)\n",
    "    s[0] = 0;\n",
    "    s[1] = 0;\n",
    "    for k in range(2, max_clusters+1):\n",
    "        kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10)\n",
    "        kmeans.fit_predict(X)\n",
    "        s[k] = metrics.silhouette_score(X, kmeans.labels_, metric='euclidean')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(range(2,len(s)),s[2:])\n",
    "    ax.set_xlabel('Number of clusters')\n",
    "    ax.set_ylabel('Silhouette coefficient')\n",
    "    ax.set_xticks(range(2, max_clusters+1))\n",
    "    \n",
    "sc_evaluate_clusters(X, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [Boston University CS591 \"Tools and Techniques for Data Mining and Applications\"](https://github.com/dataminingapp/dataminingapp-lectures)\n",
    "* https://github.com/yeseullee/Data-science-design-manual-notebooks\n",
    "* [Zaki, 2014] *Data Mining and Analysis: Fundamental Concepts and Algorithms*, by Mohammed J. Zaki and Wagner Meira : Chapters 13, 14, and 17\n",
    "* [Aggarwal, 2015] *Data Mining : The Textbook*, by Charu C. Aggarwal : Chapters 1 and 6 \n",
    "* [Skiena, 2017] *The Data Science Design Manual*, by Steven S. Skiena"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
