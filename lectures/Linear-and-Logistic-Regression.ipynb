{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy.stats as stats\n",
    "from sklearn import linear_model\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Linear regression can be used to build models based on training data:\n",
    "\n",
    "* Prediction\n",
    "* Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stock Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "smarket = pandas.read_csv('../datasets/Smarket.csv', index_col=0)\n",
    "smarket.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Where *Lag1* through *Lag5* stand for the percentage returns for the five previous days.\n",
    "\n",
    "**Prediction** : Will the index increase or decrease based on the past 5 days' percentage changes in the index ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advertising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "advertising = pandas.read_csv('../datasets/Advertising.csv', index_col=0)\n",
    "advertising.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For a particular **product**, this dataset provides for 200 markets :\n",
    "* Advertising budgets for TV, radio, and newspaper (in thousands of dollars)\n",
    "* The sales (in thousands of units)\n",
    "\n",
    "We wish to understand association between advertising and sales to control advertising expenditure in each of the three media. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Input variables** ($X$) :\n",
    "* $X_1$ : TV budget\n",
    "* $X_2$ : Radio budget\n",
    "* $X_3$ : newspaper budget\n",
    "\n",
    "**Output variable** :\n",
    "* $Y$ : Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Relationship between $Y$ and $X$ :\n",
    "\n",
    "$$\n",
    "Y = f(X) + \\epsilon\n",
    "$$\n",
    "\n",
    "* $f$ : an unkdown function\n",
    "* $\\epsilon$ : a random error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The random error term is independent of $X$ and has mean zero (otherwise easy to compensate by modifying $f$). This error cannot be reduced to zero. For example, it may depend on unmeasured variables that are useful for predicting the value of $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "advertising.plot(kind='scatter', color='Blue', x='TV', y='sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = advertising['TV']\n",
    "Y = advertising['sales']\n",
    "\n",
    "regression = np.polyfit(X, Y, deg=1)\n",
    "\n",
    "advertising.plot(kind='scatter', color='Blue', x='TV', y='sales')\n",
    "\n",
    "X = np.linspace(X.min(), X.max(), 100)\n",
    "plt.plot(X, regression[0]*X + regression[1], 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = advertising['TV']\n",
    "Y = advertising['sales']\n",
    "\n",
    "regression = np.polyfit(X, Y, deg=1)\n",
    "\n",
    "advertising.plot(kind='scatter', color='Blue', x='TV', y='sales')\n",
    "\n",
    "p = np.poly1d(regression)\n",
    "X = np.linspace(X.min(), X.max(), 100)\n",
    "plt.plot(X, p(X), 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = advertising['TV']\n",
    "Y = advertising['sales']\n",
    "\n",
    "regression = np.polyfit(X, Y, deg=3)\n",
    "\n",
    "advertising.plot(kind='scatter', color='Blue', x='TV', y='sales')\n",
    "\n",
    "p = np.poly1d(regression)\n",
    "X = np.linspace(X.min(), X.max(), 100)\n",
    "plt.plot(X, p(X), 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "Given a collection of n points, find the line which best approximates or fits the points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$\n",
    "f(x) = mx + p\n",
    "$$\n",
    "\n",
    "The residual error is defined:\n",
    "\n",
    "$$\n",
    "r_i = y_i - f(x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a random noise\n",
    "x1 = np.random.normal(size=200)\n",
    "x = np.linspace(x1.min()-1, x1.max()+1, 100)\n",
    "y = 3*(np.random.normal(0, 1, 100)+x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y , color='red')\n",
    "\n",
    "#Find the regression line\n",
    "p = np.poly1d(np.polyfit(x, y, deg=1))\n",
    "longerX = np.append(x, [5,-5])\n",
    "ax.plot(longerX, p(longerX), color='black', linewidth='1.5')\n",
    "ax.set_xlim(-4, 4.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 100)\n",
    "y = 2*x+3\n",
    "# distance the dashed line is into the circle\n",
    "s = 0.4\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "ax.plot([1, 1, 2, 3, 3, 4, 4, 5, 6, 7],\n",
    "        [7, 3, 12, 5, 11, 15, 10, 15, 12, 19], 'yo', markersize=7)\n",
    "\n",
    "_ = ax.plot([1, 1], [7-s, 5], 'b-', [1, 1], [3+s, 5], 'b-',\n",
    "            [2, 2], [12-s, 7], 'b-', [3, 3], [5+s, 9], 'b-',\n",
    "            [3, 3], [11-s, 9], 'b-', [4,4], [15-s, 11], 'b-',\n",
    "            [4,4], [10+s, 11], 'b-', [5, 5], [15-s, 13], 'b-',\n",
    "            [6, 6], [12+s+0.1, 15], 'b-', [7, 7], [19-s, 17], 'b-',\n",
    "            linestyle='dashed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem Formulation\n",
    "\n",
    "We consider a set of $n$ points $x_i = (x_{i1}, x_{i2}, \\ldots, x_{im})$, where\n",
    "$(x_{i1}, x_{i2}, \\ldots, x_{i(m-1)})$ is the feature vector and $x_{im}$ is\n",
    "the **target variable**.\n",
    "\n",
    "We use the **least squares regression** to find the optimal fit :\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n (y_i - f(x_i))^2, \\text{where}\\ f(x) = w_0 + \\sum_{j=1}^{m-1}w_jx_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrix Formulation\n",
    "\n",
    "The vector of residual values :\n",
    "\n",
    "$$\n",
    "(b - A \\cdot w),\n",
    "$$\n",
    "\n",
    "where b, A, and w are defined as follows:\n",
    "\n",
    "$$\n",
    "b = \\left[\n",
    "\\begin{array}{c}\n",
    "  x_{1m}\\\\\n",
    "  x_{2m}\\\\\n",
    "  \\vdots \\\\\n",
    "  x_{im}\\\\\n",
    "  \\vdots \\\\\n",
    "  x_{nm}\n",
    "\\end{array}\n",
    "\\right], A = \\left[\n",
    "\\begin{array}{ccccccc}\n",
    "  1 & x_{11} & x_{12} & \\ldots & x_{1j} & \\ldots & x_{1(m-1)} \\\\\n",
    "  1 & x_{21} & x_{22} & \\ldots & x_{2j} & \\ldots & x_{2(m-1)} \\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  1 & x_{i1} & x_{i2} & \\ldots & x_{ij} & \\ldots & x_{i(m-1)} \\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  1 & x_{n1} & x_{n2} & \\ldots & x_{nj} & \\ldots & x_{n(m-1)}\n",
    "\\end{array}\n",
    "\\right], w = \\left[\n",
    "\\begin{array}{c}\n",
    "  w_0\\\\\n",
    "  w_1\\\\\n",
    "  \\vdots \\\\\n",
    "  w_j \\\\\n",
    "  \\vdots \\\\\n",
    "  w_{(m-1)}\n",
    "\\end{array}\n",
    "\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimal Solution\n",
    "\n",
    "The vector $w$ that leads to the best fitting line is given by :\n",
    "\n",
    "$$\n",
    "w = (A^T A)^{-1}A^T b\n",
    "$$\n",
    "\n",
    "The right-hand side of this equation comprises the following components:\n",
    "\n",
    "* $A^T A$ : the *covariance matrix* on the features of the data matrix\n",
    "* $A^T b$ : this dot product between features and the target values measure how correlated are each feature with the targets\n",
    "\n",
    "This optimal solution is a direct consequence of the following equation :\n",
    "\n",
    "$$\n",
    "A^T (b - Aw) = \\bar{0} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax2, ax1) = plt.subplots(ncols=2, figsize=(12,4.5))\n",
    "\n",
    "# The right graph without an outlier.\n",
    "# Create a random 15 points\n",
    "np.random.seed(7)\n",
    "x1 = np.random.normal(size=10)\n",
    "x = np.linspace(x1.min()-1, x1.max()+1, 15) \n",
    "y = 3*(np.random.normal(0, 1, 15)+x)\n",
    "ax1.scatter(x, y, color='red')\n",
    "\n",
    "# Find the regression line\n",
    "regression = np.polyfit(x, y, deg=1)\n",
    "longerX = np.append(x, [5,-5]) # this makes the regression line longer\n",
    "ax1.plot(longerX, regression[0]*longerX + regression[1], color='black', linewidth='1.5')\n",
    "ax1.set_xlim(-3.5, 4.5)\n",
    "ax1.set_ylim(-15, 20)\n",
    "# The correlation coefficient value r.\n",
    "r1 = stats.pearsonr(x, y)\n",
    "ax1.set_title('Correlation coefficient = {:.2}'.format(r1[0]))\n",
    "\n",
    "# The left graph with an outlier\n",
    "# plot the 15 points with an outlier.\n",
    "x1 = np.append(x, [4])\n",
    "y1 = np.append(y, [-10])\n",
    "ax2.scatter(x1, y1, color='red')\n",
    "\n",
    "# Find the regression line\n",
    "regression = np.polyfit(x1, y1, deg=1)\n",
    "longerX = np.append(x1, [5,-5]) # this makes the regression line longer\n",
    "ax2.plot(longerX, regression[0]*longerX + regression[1], color='black', linewidth='1.5')\n",
    "ax2.set_xlim(-3.5, 4.5)\n",
    "ax2.set_ylim(-15, 20)\n",
    "# The correlation coefficient value r.\n",
    "r2 = stats.pearsonr(x1, y1)\n",
    "ax2.set_title('Correlation coefficient = {:.2}'.format(r2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least square regression are sensitive to outliers, because they have a large impact on the following objective function :\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n (y_i - f(x_i))^2\n",
    "$$\n",
    "\n",
    "For example, compare the impact of a point at distance 10 and a second point at distance 1 from the line.\n",
    "\n",
    "A simple solution for dealing with such points, is to first compute a linear regression on the complete data set. Then, compare residual values $r_i = (y_i - f(x_i))^2$ to determine which points are outliers in the dataset. Finaly, compute a new linear regression without these points. However, are we sure these points represent errors ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Non-Linear Functions\n",
    "\n",
    "How to fit a quadratic model ? \n",
    "\n",
    "$$\n",
    "y = w_0 + w_1 x + w_2 x^2\n",
    "$$\n",
    "\n",
    "Add a new column to the data matrix equal to $x^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,4.5))\n",
    "\n",
    "n = 20\n",
    "x = np.linspace(0, n, n) \n",
    "y = np.random.normal(0, 10, n) + 0.5 * x**2\n",
    "ax.scatter(x, y, color='red')\n",
    "\n",
    "# Fit the model\n",
    "A = np.vstack((np.ones(len(x)), x, x**2)).T\n",
    "w = np.linalg.lstsq(A, y)[0]\n",
    "\n",
    "# Predict\n",
    "x = np.linspace(-5, n+5, 10*n) \n",
    "A = np.vstack((np.ones(len(x)), x, x**2)).T\n",
    "ax.plot(x, np.dot(A, w), 'b')\n",
    "ax.set_xlim(-5, n+5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "Let us consider the following prediction problem :\n",
    "\n",
    "* $y$ : Gross national product (in dollars)\n",
    "* $x_1$ : Population size\n",
    "* $x_2$ : Literacy rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2015'\n",
    "cols = ['Country Name', year]\n",
    "gdp = pandas.read_csv('../datasets/API_NY.GDP.MKTP.CD_DS2_en_csv_v2.csv',\n",
    "                      index_col=0, skiprows=4, usecols=cols)\n",
    "gdp.rename(columns={year : 'GDP'}, inplace=True);\n",
    "gdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "literacy = pandas.read_csv('../datasets/API_SE.ADT.LITR.ZS_DS2_en_csv_v2.csv',\n",
    "                           index_col=0, skiprows=4, usecols=cols)\n",
    "literacy.rename(columns={year : 'Literacy'}, inplace=True);\n",
    "literacy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = pandas.read_csv('../datasets/API_SP.POP.TOTL_DS2_en_csv_v2.csv',\n",
    "                               index_col=0, skiprows=4, usecols=cols)\n",
    "population.rename(columns={year : 'Population'}, inplace=True);\n",
    "population.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.concat([population, gdp, literacy], axis=1)\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "A = np.vstack((np.ones(df.shape[0]), df['Population'], df['Literacy'])).T\n",
    "w, residuals = np.linalg.lstsq(A, df['GDP'])[:2]\n",
    "print(w)\n",
    "print(residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(df['Population'], df['Literacy'], df['GDP'], color='r')\n",
    "\n",
    "X = np.linspace(np.min(df['Population']), np.max(df['Population']), 20)\n",
    "Y = np.linspace(np.min(df['Literacy']), np.max(df['Literacy']), 20)\n",
    "\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "ax.plot_wireframe(X, Y, w[0] + X*w[1] + Y*w[2], linewidth=1, antialiased=True)\n",
    "\n",
    "ax.set_xlabel('Population')\n",
    "ax.set_ylabel('Literacy')\n",
    "ax.set_zlabel('GDP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues in such models :\n",
    "    \n",
    "* Unreadable coefficients\n",
    "* Numerical imprecision\n",
    "* Inapproprite formulations\n",
    "\n",
    "Scaling operations are used to address these problems.\n",
    "\n",
    "* **Z-Scores** : $Z(x) = (x - \\mu)/\\sigma$\n",
    "* **Sublinear Scaling** for *power law distributed* features : $\\log(x), \\sqrt{x},...$\n",
    "* Combine these transformations : for example, *Z-Scores* should be applied on features transformed by sublinear functions\n",
    "* Apply scaling on target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "x1 = stats.zscore(df['Population'])\n",
    "x2 = stats.zscore(df['Literacy'])\n",
    "y = stats.zscore(df['GDP'])\n",
    "\n",
    "A = np.vstack((np.ones(df.shape[0]), x1 , x2)).T\n",
    "w, residues = np.linalg.lstsq(A, y)[:2]\n",
    "print(w)\n",
    "print(residues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "x1 = np.log(df['Population'])\n",
    "x2 = np.log(df['Literacy'])\n",
    "y = np.log(df['GDP'])\n",
    "\n",
    "A = np.vstack((np.ones(df.shape[0]), x1 , x2)).T\n",
    "w, residues = np.linalg.lstsq(A, y)[:2]\n",
    "print(w)\n",
    "print(residues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(x1, x2, y, color='r')\n",
    "\n",
    "X = np.linspace(np.min(x1), np.max(x1), 20)\n",
    "Y = np.linspace(np.min(x2), np.max(x2), 20)\n",
    "\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = w[0] + X*w[1] + Y*w[2]\n",
    "\n",
    "ax.plot_wireframe(X, Y, Z, linewidth=1, antialiased=True)\n",
    "\n",
    "ax.set_xlabel('Population')\n",
    "ax.set_ylabel('Literacy')\n",
    "ax.set_zlabel('GDP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(x1, x2, np.exp(y), color='r')\n",
    "\n",
    "X = np.linspace(np.min(x1), np.max(x1), 20)\n",
    "Y = np.linspace(np.min(x2), np.max(x2), 20)\n",
    "\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = np.exp(w[0] + X*w[1] + Y*w[2])\n",
    "\n",
    "ax.plot_wireframe(X, Y, Z, linewidth=1, antialiased=True)\n",
    "\n",
    "ax.set_xlabel('Population')\n",
    "ax.set_ylabel('Literacy')\n",
    "ax.set_zlabel('GDP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlated Features\n",
    "\n",
    "* Problem in numerical methods. For example, the covariance matrix $(A^T A)$ might be *singular* which is required to be inversible for computing the linear regression.\n",
    "\n",
    "* Solution : eliminate or combine strongly correlated features by computing the covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The right graph without an outlier.\n",
    "# Create a random 15 points\n",
    "np.random.seed(7)\n",
    "n = 100\n",
    "x1 = np.random.normal(size=n)\n",
    "x2 = np.random.normal(size=n) * 0.1\n",
    "y = np.random.normal(size=n) * 0.1 + 3*x1 + 2\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(x1, x2, y, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(np.vstack((x1, x2, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.vstack((np.ones(n), x1 , x2)).T\n",
    "w, residues = np.linalg.lstsq(A, y)[:2]\n",
    "print(w, residues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.vstack((np.ones(n), x1)).T\n",
    "w, residues = np.linalg.lstsq(A, y)[:2]\n",
    "print(w, residues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single variable model is better. Why ?\n",
    "\n",
    "**Occam's Razor** : *\"the simplest explanation is the best explanation\"*\n",
    "* The two first models are equally accurate, therefore we should use the simpler one.\n",
    "\n",
    "**Observations** :\n",
    "\n",
    "* The second variable is *noise* and only fit the noise in the measured target values $y$.\n",
    "* Coefficients of uncorelated features (in this case $x_2$) in the linear model are close to zero. \n",
    "* Our evaluation leads to **overfitting** : models have to be evaluated with respect to the test set !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the use of correlated variables : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x3 = x1 + (np.random.normal(size=n) * 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.vstack((np.ones(len(x1)), x1, x3)).T\n",
    "w, residues = np.linalg.lstsq(A, y)[:2]\n",
    "print(w, residues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model becomes very sensible to random errors :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errors = np.random.normal(size=n) * 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare how evolve the two models :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.vstack((np.ones(len(x1)), x1, x3)).T\n",
    "w, residues = np.linalg.lstsq(A, y)[:2]\n",
    "print(w, residues)\n",
    "\n",
    "# with random errors\n",
    "A = np.vstack((np.ones(len(x1)), x1 + errors, x3)).T\n",
    "w, residues = np.linalg.lstsq(A, y)[:2]\n",
    "print(w, residues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.vstack((np.ones(n), x1)).T\n",
    "w, residues = np.linalg.lstsq(A, y)[:2]\n",
    "print(w, residues)\n",
    "\n",
    "# with random errors\n",
    "A = np.vstack((np.ones(len(x1)), x1 + errors)).T\n",
    "w, residues = np.linalg.lstsq(A, y)[:2]\n",
    "print(w, residues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Use the smallest (and best) set of features !\n",
    "\n",
    "For this purpose we can use *regularization* techniques such as **Ridge Regression** that adds a penaly in the objective function :\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{2n} \\sum_{i = 1}^n (y_i - f(x_i))^2 + \\lambda \\sum_{j = 1}^m w_j^2\n",
    "$$\n",
    "\n",
    "where $\\lambda$ that allows to strength of the regularization in order to find a *compromise* between the residue and small coefficients ($w$).\n",
    "\n",
    "*It is worth noting that such an approach is meaningfull only if the features are normalized so that coefficients magnitudes $w_j^2$ properly measure the the importance of features in the objective function.*\n",
    "\n",
    "See Ridge Regression from scikit-learn : http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.vstack((x1, x3)).T\n",
    "reg = linear_model.Ridge()\n",
    "reg.fit(A, y)\n",
    "print( reg.intercept_, reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.vstack((x1 + errors, x3)).T\n",
    "reg = linear_model.Ridge()\n",
    "reg.fit(A, y)\n",
    "print( reg.intercept_, reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This quadratic objective function does obviously not penalize proportionaly (linearly) small and large coefficients. This leads to only select small coefficents, but does not lead to remove some features (by assigning zero coeficients). For this purpose, we can use a **LASSO regression** (that stands for \"Least Absolute Shrinkage and Selection Operator\") that moves the penalty to the contraints of the optimization problem :\n",
    "\n",
    "$$\n",
    "J(W, t) = \\frac{1}{2n} \\sum_{i = 1}^n (y_i - f(x_i))^2,\\ \\text{subject to}\\ \\sum_{j = 1}^m \\left|w_j\\right| \\leqslant t.\n",
    "$$\n",
    "\n",
    "See LASSO from scikit-learn : http://scikit-learn.org/stable/modules/linear_model.html#lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.vstack((x1, x3)).T\n",
    "reg = linear_model.Lasso(alpha = .01)\n",
    "reg.fit(A, y)\n",
    "print( reg.intercept_, reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.vstack((x1 + errors, x3)).T\n",
    "reg = linear_model.Lasso(alpha = .01)\n",
    "reg.fit(A, y)\n",
    "print( reg.intercept_, reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** :\n",
    "\n",
    "* How do we choose the right value of $t$ or $\\lambda$ ?\n",
    "* Do we have to use ridge regression or LASSO regression ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = [-14,-12, -10, -8, -7, -4]\n",
    "x1 = [3, 5, 7, 10, 12, 13]\n",
    "y = [0]*len(x)\n",
    "y1 = [1]*len(x1)\n",
    "regline = np.polyfit(x+x1, y+y1,1)\n",
    "\n",
    "ax.plot(x, y,'bo', x1, y1,'ro')\n",
    "\n",
    "ax.set_ylim(-0.2, 1.2)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = [-14,-12, -10, -8, -7, -4]\n",
    "x1 = [3, 5, 7, 10, 12, 13]\n",
    "y = [0]*len(x)\n",
    "y1 = [1]*len(x1)\n",
    "regline = np.polyfit(x+x1, y+y1,1)\n",
    "\n",
    "ax.plot(x, y,'bo', x1, y1,'ro')\n",
    "ax.plot(x+x1, [regline[0]*i + regline[1] for i in x+x1], 'k-', linewidth=2)\n",
    "ax.plot([0,0], [1.2,-0.2], 'g-', linewidth=2)\n",
    "ax.plot([-15,15], [0.5,0.5], 'k--')\n",
    "\n",
    "ax.set_ylim(-0.2, 1.2)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundaries\n",
    "\n",
    "**Binary classification** problems :\n",
    "\n",
    "* Spam = 1 / Non-Spam = 0\n",
    "* Cancer = 1 / Benign = 0\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "np.random.seed(6) # fix random values\n",
    "x1 = np.random.normal(5, 2, 10)\n",
    "y1 = np.random.normal(5, 2, 10)\n",
    "x2 = np.random.normal(10, 2, 10)\n",
    "y2 = np.random.normal(10, 2, 10)\n",
    "ax.plot(x1, y1,'bo', x2, y2,'ro')\n",
    "ax.plot([1.6, 12.6], [14, 3.4], 'g-', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we assess the quality of the resulting separation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "np.random.seed(4) # fix random values\n",
    "x1 = np.random.normal(5, 2, 10)\n",
    "y1 = np.random.normal(5, 2, 10)\n",
    "x2 = np.random.normal(10, 2, 10)\n",
    "y2 = np.random.normal(10, 2, 10)\n",
    "ax.plot(x1, y1,'bo', x2, y2,'ro')\n",
    "ax.plot([1.6, 12.6], [14, 3.4], 'g-', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(-10,10,100)\n",
    "y = 1/(1+np.exp(-x))\n",
    "ax.plot(x, y, linewidth='1.7')\n",
    "ax.set_ylim(-0.001,1.01)\n",
    "ax.text(-10, 0.7, r'$f(x) = \\frac{1}{1+e^{-x}}$', fontsize=18, fontweight='bold')\n",
    "ax.set_title('The logistic function $f(x)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logit(ax, x, c, t):\n",
    "    y = 1/(1+np.exp(-c*x + t))\n",
    "    ax.plot(x, y, linewidth='1.7')\n",
    "    ax.set_ylim(-0.001,1.01)\n",
    "    ax.text(-10, 0.7, r'$\\frac{1}{1+e^{-' + str(c) + 'x + ' + str(t) + '}}$', fontsize=18, fontweight='bold')\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2)\n",
    "x = np.linspace(-10,10,100)\n",
    "plot_logit(axs[0,0], x, 1, 0)\n",
    "plot_logit(axs[0,1], x, 3, 0)\n",
    "plot_logit(axs[1,0], x, 1, 5)\n",
    "plot_logit(axs[1,1], x, 3, 10)\n",
    "fig.suptitle('$f(x) = 1/(1+e^{-cx + t})$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function $f : \\mathbb{R} \\rightarrow[0, 1]$ can be used to determine the *probability* $f(x)$ that a given item at distance $x$ from the boundary has to get the positive label (1 is said \"positive\" and 0 \"negative\").\n",
    "\n",
    "We need to build such a model that allows to :\n",
    "\n",
    "* Take into account the $(m-1)$ features \n",
    "* Choose a suitable value of $c$ (defining the *steepness* of the transition between the two classes)\n",
    "* Choose a suitable value of $t$ \n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-h(x, w)}},\\ \\text{where}\\ h(x, w) = w_0 + \\sum_{i = 1}^{m-1} w_i \\cdot x_i\n",
    "$$\n",
    "\n",
    "where $t$ and $c$ are defined by $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalty function for misclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J(w) = \\frac{1}{n} \\sum_{i = 1}^{n} cost(f(x_i, w), y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0.01, 0.99,100)\n",
    "y1 = -np.log(x)\n",
    "y2 = -np.log(1-x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, y1, 'r-', label=r\"$-\\log(h_0(x))$, $y=1$\", linewidth=2)\n",
    "ax.plot(x, y2, 'b-', label=r\"$-\\log(1-h_0(x))$, $y=0$\", linewidth=2)\n",
    "\n",
    "ax.set_xlabel(r'$h_0(x)$')\n",
    "ax.set_ylabel(r\"$Cost(h_0(x), y)$\")\n",
    "ax.legend(loc=\"upper center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues in Logistic Classification\n",
    "\n",
    "**Unbalanced training set**\n",
    "\n",
    "Consider a dataset composed by 99.9% items from class-0 and the remaining 0.1% items from class-1\n",
    "\n",
    "* What happens if all the class-1 items are misclassified ?\n",
    "* Would 0.1% of false positives be acceptable ?\n",
    "* The contribution to the loss function would not be fair, *i.e.* the majority class is favored.\n",
    "\n",
    "Solutions ?\n",
    "\n",
    "* Discard members of the bigger class.\n",
    "* Duplicate items from the smaller class with random perturbation to avoid overfitting *(otherwise, the classification of all the duplicated items would the same leading to a strong constraint on the model with respect to the objective function, because misclassifying an item leads to misclassifying all its duplicates)*.\n",
    "* Add more weights on items from the minority class (can lead to overfitting...)\n",
    "\n",
    "**Multi-class classification**\n",
    "\n",
    "Ordinal Scale (ratings : 1, 2, 3, 4 stars) \n",
    "\n",
    "* Linear regression\n",
    "\n",
    "Nominal Scale (genres  : drama, comedy,...)\n",
    "\n",
    "* Multiple one-vs.-rest classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [An Introduction to Statistical Learning with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/) by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani\n",
    "* **The Data Science Design Manual**, by Steven Skiena, 2017, Springer\n",
    "* Python notebooks available at [http://data-manual.com/data](http://data-manual.com/data)\n",
    "* Lectures slides available at [http://www3.cs.stonybrook.edu/~skiena/data-manual/lectures/](http://www3.cs.stonybrook.edu/~skiena/data-manual/lectures/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
