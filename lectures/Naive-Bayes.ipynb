{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\*#1{\\mathbf{#1}}$\n",
    "$\\DeclareMathOperator*{\\argmax}{arg\\,max}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Models : the case of the Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import log\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "\n",
    "A classifier is a function $M$ that predicts the class label $\\hat{y} \\in \\{c_1, c_2, \\ldots, c_k\\}$ for a given point $\\*x \\in \\mathbb{R}^d$ :\n",
    "\n",
    "$$\\hat{y} = M(\\*x)$$\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "1. Collect a **Training Data Set** composed of points along with their known classes\n",
    "2. Train Classifier\n",
    "3. Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Training Data\n",
    "\n",
    "A training set of points with their known classes :\n",
    "\n",
    "| Day | Outlook | Temp | Humidity | Beach? |\n",
    "|-----|---------|------|----------|--------|\n",
    "| 1   | Sunny   | High | High     | Yes    |\n",
    "| 2   | Sunny   | High | Normal   | Yes    |\n",
    "| 3   | Sunny   | Low  | Normal   | No     |\n",
    "| 4   | Sunny   | Mild | High     | Yes    |\n",
    "| 5   | Rain    | Mild | Normal   | No     |\n",
    "| 6   | Rain    | High | High     | No     |\n",
    "| 7   | Rain    | Low  | Normal   | No     |\n",
    "| 8   | Cloudy  | High | High     | No     |\n",
    "| 9   | Cloudy  | High | Normal   | Yes    |\n",
    "| 10  | Cloudy  | Mild | Normal   | No     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Classification\n",
    "\n",
    "We consider a dataset $D = \\{\\*x_1, \\ldots, \\*x_n\\}$, with $y_i \\in \\{c_1, c_2, \\ldots, c_k\\}$ for all $i = 1, 2, \\ldots, n$.\n",
    "\n",
    "The **predicted class** for a new data point $\\*x$ is obtained as follows :\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\argmax_{c_i}\\{P(c_i | \\*x)\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' Theorem\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "This allows to reduce question such as P(outcome|data) to P(data|outcome)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Classifier\n",
    "\n",
    "Based on this formula, The **predicted class** for a new data point $\\*x$ is obtained as follows :\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y} &= \\argmax_{c_i}\\{P(c_i | \\*x)\\}\\\\\n",
    "        &= \\argmax_{c_i}\\Bigg\\{\\frac{P(\\*x | c_i) P(c_i)}{P(\\*x)}\\Bigg\\}\\\\\n",
    "        &= \\argmax_{c_i}\\{P(\\*x | c_i) P(c_i)\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Prior Probability Estimator** :\n",
    "\n",
    "$$\n",
    "\\hat{P}(c_i) = \\frac{|\\*D_i|}{|\\*D|} = \\frac{n_i}{n}\n",
    "$$\n",
    "\n",
    "where $\\*D_i = \\{\\*x_j \\in \\*D : y_j = c_i\\}$, $|\\*D| = n$, and $|\\*D_i| = n_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier\n",
    "In this case we assume that attributes are independent. This allows to write the likelyhood as follows :\n",
    "    \n",
    "$$\n",
    "P(\\*x|c_i) = P(x_1, x_2, \\ldots, x_n | c_i) = \\prod_{j = 1}^d P(x_i|c_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **categorical attributes**, \n",
    "\n",
    "$$\n",
    "P(\\*x|c_i) = \\prod_{j = 1}^d P(x_i|c_i) \\approx \\prod_{j = 1}^d \\hat{f}(\\*v_j | c_i) = \\prod_{j = 1}^d \\frac{n_i(\\*v_j)}{n_i}\n",
    "$$\n",
    "\n",
    "where $n_i(\\*v_j)$ is the observed frequency of the value $\\*v_j$ for the attribute $X_i$ in the class $c_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Day | Outlook | Temp | Humidity | Beach? |\n",
    "|-----|---------|------|----------|--------|\n",
    "| 1   | Sunny   | High | High     | Yes    |\n",
    "| 2   | Sunny   | High | Normal   | Yes    |\n",
    "| 3   | Sunny   | Low  | Normal   | No     |\n",
    "| 4   | Sunny   | Mild | High     | Yes    |\n",
    "| 5   | Rain    | Mild | Normal   | No     |\n",
    "| 6   | Rain    | High | High     | No     |\n",
    "| 7   | Rain    | Low  | Normal   | No     |\n",
    "| 8   | Cloudy  | High | High     | No     |\n",
    "| 9   | Cloudy  | High | Normal   | Yes    |\n",
    "| 10  | Cloudy  | Mild | Normal   | No     |\n",
    "\n",
    "| Outlook      | Beach | No Beach |\n",
    "|--------------|-------|----------|\n",
    "| Sunny        | 3/4   | 1/6      |\n",
    "| Rain         | 0/4   | 3/6      |\n",
    "| Cloudy       | 1/4   | 2/6      |\n",
    "\n",
    "| Temperature  | Beach | No Beach |\n",
    "|--------------|-------|----------|\n",
    "| High         | 3/4   | 2/6      |\n",
    "| Mild         | 1/4   | 2/6      |\n",
    "| Low          | 0/4   | 2/6      |\n",
    "\n",
    "| Humidity     | Beach | No Beach |\n",
    "|--------------|-------|----------|\n",
    "| High         | 2/4   | 2/6      |\n",
    "| Normal       | 2/4   | 4/6      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, based on the wather condition (Sunny, Mild, High), would this be a day to go to the beach ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Zero counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we consider (Sunny, Mild, Low), where the value of Low Humidity dow not appear in the training set ?\n",
    "\n",
    "* P(Beach|(Sunny, Mild, Low)) = 0\n",
    "* P(No Beach|(Sunny, Mild, Low)) = 0\n",
    "\n",
    "Add-one **discounting** is a traditional statistical technique to address this problem : add one to the frequency of all outcomes. By applying this rules, the probabilities become what follows.\n",
    "\n",
    "| Outlook      | Beach | No Beach |\n",
    "|--------------|-------|----------|\n",
    "| Sunny        | 4/7   | 2/9      |\n",
    "| Rain         | 1/7   | 4/9      |\n",
    "| Cloudy       | 2/7   | 3/9      |\n",
    "\n",
    "| Temperature  | Beach | No Beach |\n",
    "|--------------|-------|----------|\n",
    "| High         | 4/7   | 3/9      |\n",
    "| Mild         | 2/7   | 3/9      |\n",
    "| Low          | 1/7   | 3/9      |\n",
    "\n",
    "| Humidity     | Beach | No Beach |\n",
    "|--------------|-------|----------|\n",
    "| High         | 3/7   | 3/9      |\n",
    "| Normal       | 3/7   | 5/9      |\n",
    "| Low          | 1/7   | 1/9      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate bernoulli models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sunny, Rain, Cloudy, Low, Mild, High, Normal, High\n",
    "data = np.array([\n",
    "    [1, 0, 0, 0, 0, 1, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 1, 1, 0],\n",
    "    [1, 0, 0, 1, 0, 0, 1, 0],\n",
    "    [1, 0, 0, 0, 1, 0, 0, 1],\n",
    "    [0, 1, 0, 0, 1, 0, 1, 0],\n",
    "    [0, 1, 0, 0, 0, 1, 0, 1],\n",
    "    [0, 1, 0, 1, 0, 0, 1, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 1],\n",
    "    [0, 0, 1, 0, 0, 1, 1, 0],\n",
    "    [0, 0, 1, 0, 1, 0, 1, 0]])\n",
    "\n",
    "target = np.array([1, 1, 0, 1, 0, 0, 0, 0, 1, 0])\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(data, target)\n",
    "bnb.predict_proba([[1, 0, 0, 0, 1, 0, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model assume independence between some variables which is obviously wrong. Why ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB(alpha=1.0e-10)\n",
    "mnb.fit(data, target)\n",
    "mnb.predict_proba([[1, 0, 0, 0, 1, 0, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Attributes\n",
    "\n",
    "For *numerical attributes*, we can, for example, assume a normal distribution for each clas $c_i$ :\n",
    "\n",
    "$$\n",
    "P(x_j | c_i) \\propto f(x_j) = \\frac{1}{\\sqrt{2\\sigma_{ij}^2\\pi} } \\; e^{ -\\frac{(x_-\\mu_{ij})^2}{2\\sigma_{ij}^2} }\n",
    "$$\n",
    "\n",
    "where $\\mu_{ij}$ and $\\sigma_{ij}^2$ denote the mean and variance for attribute $X_j$ and class $c_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(iris.data.shape[0],\n",
    "                                                                         (iris.target != y_pred).sum()))                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Evaluation\n",
    "\n",
    "### Performance Measure\n",
    "\n",
    "* $\\*D$ : the data set composed of $n$ points in a $d$ dimensional space\n",
    "* $k$ : the number of classes\n",
    "* $M$ : the classifier\n",
    "* $y_i$ : the \"true\" class that corresponds to $\\*x_i \\in \\*D$\n",
    "* $\\hat{y}_i = M(\\*x_i)$ : the predicted class\n",
    "    \n",
    "#### Error Rate\n",
    "\n",
    "The fraction of incorrect predictions :\n",
    "\n",
    "$$\n",
    "Error\\ Rate = \\frac{1}{n} \\sum_{i = 1}^n I(y_i \\neq \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "#### Accuracy\n",
    "\n",
    "The fraction of correct predictions :\n",
    "$$\n",
    "Accuracy = \\frac{1}{n} \\sum_{i = 1}^n I(y_i = \\hat{y}_i) = 1 - Error\\ Rate\n",
    "$$\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "The data set $\\*D$ is split into two disjoint sets :\n",
    "\n",
    "* **Training set** : used to train $M$\n",
    "* **Testing set** : used to evaluate the performance measure (denoted $\\theta$) of $M$\n",
    "\n",
    "#### $K$-fold Cross-Validation\n",
    "\n",
    "The data set $\\*D$ is divided into $K$ equal-sized parts, denoted $\\*D_1, \\*D_2, \\ldots, \\*D_K$, called folds.\n",
    "\n",
    "For each $i = 1,2,\\ldots,K$, a model $M_i$ is trained and the corresponding measure $\\theta_i$ is evaluated with respect to :\n",
    "\n",
    "* $\\*D_i$ : the testing set\n",
    "* $\\*D \\backslash \\*D_i = \\cup_{j \\neq i} \\*D_j$ : the training set\n",
    "\n",
    "This lead estimate the expected and variance values of the performance measure as follows,\n",
    "\n",
    "* $\\displaystyle \\hat{\\mu}_{\\theta} = E[\\theta] = \\frac{1}{K} \\sum_{i=1}^K \\theta_i$\n",
    "* $\\displaystyle \\hat{\\sigma}_{\\theta}^2 = \\frac{1}{K} \\sum_{i=1}^K (\\theta_i - \\hat{\\mu}_{\\theta})^2$\n",
    "\n",
    "When $K = 1$ this validation method is called the **leave-one-out** cross-validation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References :\n",
    "\n",
    "* **The Data Science Design Manual**, by Steven Skiena, 2017, Springer;\n",
    "* Python notebooks available at [http://data-manual.com/data](http://data-manual.com/data);\n",
    "* Lectures slides available at [http://www3.cs.stonybrook.edu/~skiena/data-manual/lectures/](http://www3.cs.stonybrook.edu/~skiena/data-manual/lectures/).\n",
    "* The Google Developers channel : Machine Learning Recipes\n",
    "* Spring 2015 Boston University CS591 \"Tools and Techniques for Data Mining and Applications\" course "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
